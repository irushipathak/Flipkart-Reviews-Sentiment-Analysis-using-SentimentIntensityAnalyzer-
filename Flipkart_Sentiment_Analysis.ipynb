{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97f1d993",
   "metadata": {},
   "source": [
    "# Flipkart Product Reviews â€” Sentiment Analysis (VADER)\n",
    "\n",
    "This notebook loads Flipkart reviews, computes VADER sentiment scores, preprocesses text, trains a simple classifier, and evaluates results. Update the `DATA_PATH` cell to point to your CSV file (place `FlipkartData.csv` in the path you choose)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdf73cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "print('NLTK downloads complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1c73a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "print('Libraries imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441f8baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = r'D:\\\\projects\\\\FlipkartData.csv'  # <-- change if needed\n",
    "\n",
    "df = pd.read_csv(DATA_PATH, encoding='ISO-8859-1', low_memory=False)\n",
    "print('Loaded shape:', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65165bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_cols = ['Summary', 'Sentiment']\n",
    "for c in required_cols:\n",
    "    if c not in df.columns:\n",
    "        raise ValueError(f\"Dataset must contain column: {c}\")\n",
    "        \n",
    "# Normalize Sentiment\n",
    " df['Sentiment'] = df['Sentiment'].astype(str).str.strip().str.lower()\n",
    "print('Sentiment value counts:')        \n",
    "print(df['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0450453",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "all_stop = set(stopwords.words('english'))\n",
    "neg_keep = {\"no\",\"not\",\"nor\",\"don't\",\"didn't\",\"doesn't\",\"isn't\",\"wasn't\",\"weren't\",\"won't\",\"can't\",\"couldn't\",\"shouldn't\",\"wouldn't\",\"cannot\"}\n",
    "stopwords_to_remove = all_stop - neg_keep\n",
    "\n",
    "def preprocess(text):\n",
    "    text = str(text)\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    tokens = text.lower().split()\n",
    "    tokens = [ps.stem(t) for t in tokens if t not in stopwords_to_remove]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Drop rows with empty summaries\n",
    " df = df[df['Summary'].notna()].copy()\n",
    " df['clean'] = df['Summary'].apply(preprocess)\n",
    " df[['Summary','clean']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232b46b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "df['vader_scores'] = df['clean'].apply(lambda t: sia.polarity_scores(str(t)))\n",
    "# expand to columns\n",
    "vader_df = pd.DataFrame(df['vader_scores'].tolist())\n",
    "df = pd.concat([df.reset_index(drop=True), vader_df.reset_index(drop=True)], axis=1)\n",
    "df[['Summary','clean','compound','pos','neu','neg']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c74ab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Sentiment'].isin(['positive','negative'])].copy()\n",
    "df['label'] = df['Sentiment'].map({'positive':1,'negative':0})\n",
    "\n",
    "cv = CountVectorizer(max_features=3000, ngram_range=(1,2))\n",
    "X = cv.fit_transform(df['clean']).toarray()\n",
    "y = df['label'].values\n",
    "print('X shape:', X.shape, 'y shape:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683beda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=42)\n",
    "\n",
    "clf = MultinomialNB(alpha=1.0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27340fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('\\nClassification report:\\n', classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion matrix:\\n', cm)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['pred_neg','pred_pos'], yticklabels=['true_neg','true_pos'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd941fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('flipkart_with_vader_and_clean.csv', index=False)\n",
    "print('Saved flipkart_with_vader_and_clean.csv')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
